{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48dfd5d",
   "metadata": {},
   "source": [
    "## Stacked Transformer and Linear Regression - Weighted Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef82747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Parameters\n",
    "stock_symbol = 'AAME' \n",
    "start_date = '2013-01-01'\n",
    "end_date = '2023-01-01'\n",
    "window_size = 10  \n",
    "\n",
    "# Fetch data from Yahoo Finance\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "prices = data['Close'].values\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(data, window):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i:i + window])\n",
    "        y.append(data[i + window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(prices, window_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=1, num_heads=1, ffn_hid_dim=128):  \n",
    "        super().__init__()\n",
    "        self.model_dim = input_dim\n",
    "        self.pos_encoder = nn.Linear(input_dim, self.model_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.model_dim, nhead=num_heads, dim_feedforward=ffn_hid_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(self.model_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x * np.sqrt(self.model_dim)  # Scale embedding\n",
    "        x = x.permute(1, 0, 2)  # Shape to [seq_len, batch_size, features]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Shape back to [batch_size, seq_len, features]\n",
    "        return self.fc_out(x[:, -1, :]).squeeze(-1)\n",
    "\n",
    "# Initialize the model with the correct input dimension\n",
    "transformer_model = TransformerModel(input_dim=1)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(-1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "\n",
    "# Train Linear Regression\n",
    "linear_model = LinearRegression().fit(X_train_scaled.reshape(-1, window_size), y_train)\n",
    "\n",
    "# Predictions\n",
    "transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    transformer_preds = transformer_model(X_test_tensor).numpy()\n",
    "\n",
    "linear_preds = linear_model.predict(X_test_scaled)\n",
    "ensemble_preds = (transformer_preds + linear_preds) / 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, ensemble_preds)\n",
    "mae = mean_absolute_error(y_test, ensemble_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - ensemble_preds) / y_test)) * 100\n",
    "print(f'Ensemble MSE: {mse}')\n",
    "print(f\"Ensemble MAE: {mae}\")\n",
    "print(f\"Ensemble RMSE: {rmse}\")\n",
    "print(f\"Ensemble MAPE: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a401c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate R2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, ensemble_preds)\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc6884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "stock_symbol = 'AAME'\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2024-01-01'\n",
    "window_size = 10  \n",
    "\n",
    "# Fetch data from Yahoo Finance\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "prices = data['Close'].values\n",
    "\n",
    "# Function to create sequences for training\n",
    "def create_sequences(data, window):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i:i + window])\n",
    "        y.append(data[i + window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(prices, window_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Transformer Model Definition\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=1, num_heads=1, ffn_hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.model_dim = input_dim\n",
    "        self.pos_encoder = nn.Linear(input_dim, self.model_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.model_dim, nhead=num_heads, dim_feedforward=ffn_hid_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(self.model_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x * np.sqrt(self.model_dim)  # Scale embedding\n",
    "        x = x.permute(1, 0, 2)  # Shape to [seq_len, batch_size, features]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Shape back to [batch_size, seq_len, features]\n",
    "        return self.fc_out(x[:, -1, :]).squeeze(-1)\n",
    "\n",
    "# Initialize and prepare models\n",
    "transformer_model = TransformerModel(input_dim=1)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(-1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader setup\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training function for the Transformer model\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=100):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        training_losses.append(total_loss / len(train_loader))\n",
    "    return training_losses\n",
    "\n",
    "# Train the Transformer model\n",
    "transformer_losses = train_model(transformer_model, train_loader, optimizer, criterion)\n",
    "\n",
    "# Train Linear Regression model\n",
    "linear_model = LinearRegression().fit(X_train_scaled.reshape(-1, window_size), y_train)\n",
    "\n",
    "# Ensemble prediction function\n",
    "def weighted_ensemble(transformer_model, linear_model, X_test_tensor, X_test_scaled, transformer_weight=0.70):\n",
    "    transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        transformer_preds = transformer_model(X_test_tensor).numpy()\n",
    "    linear_preds = linear_model.predict(X_test_scaled)\n",
    "    ensemble_preds = (transformer_preds * transformer_weight) + (linear_preds * (1 - transformer_weight))\n",
    "    return ensemble_preds\n",
    "\n",
    "# Generate ensemble predictions\n",
    "ensemble_preds = weighted_ensemble(transformer_model, linear_model, X_test_tensor, X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_preds)\n",
    "print(f'Ensemble MSE: {ensemble_mse}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Actual Values', alpha=0.7)\n",
    "plt.plot(ensemble_preds, label='Ensemble Predictions', alpha=0.7)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual Prices vs. Ensemble Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7193f3",
   "metadata": {},
   "source": [
    "## ARIMA and Stacked TRansformer - Weighted Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52ae300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = ARIMA(y_train, order=(2,1,0))  \n",
    "arima_fitted = arima_model.fit()\n",
    "\n",
    "# Predictions using ARIMA on the test set\n",
    "arima_preds = arima_fitted.forecast(steps=len(X_test))\n",
    "\n",
    "# Using the weights\n",
    "transformer_weight = 0.20  # Adjusted weight to Transformer\n",
    "arima_weight = 0.80      # Adjusted weight to ARIMA predictions (not linear regression anymore)\n",
    "\n",
    "# Make sure weights sum to 1\n",
    "assert transformer_weight + arima_weight == 1, \"Weights must sum to 1.\"\n",
    "\n",
    "# Load Transformer predictions \n",
    "transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    transformer_preds = transformer_model(X_test_tensor).numpy()\n",
    "\n",
    "# Calculate the weighted average of predictions\n",
    "ensemble_preds = (transformer_preds * transformer_weight) + (arima_preds * arima_weight)\n",
    "\n",
    "# Evaluate performance\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_preds)\n",
    "print(f'Ensemble MSE: {ensemble_mse}')\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot training losses for Transformer\n",
    "ax[0].plot(transformer_losses, label='Transformer Training Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training Loss per Epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "ax[1].plot(y_test, label='Actual Values', alpha=0.7)\n",
    "ax[1].plot(ensemble_preds, label='Ensemble Predictions', alpha=0.7)\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].set_ylabel('Stock Price')\n",
    "ax[1].set_title('Comparison of Actual Prices vs. Ensemble Predictions')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8478c35",
   "metadata": {},
   "source": [
    "## Stacked Transformer and Linear Regression - Weighted Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d65d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "stock_symbol = 'AAL'\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2024-01-01'\n",
    "window_size = 10  \n",
    "\n",
    "# Fetch data from Yahoo Finance\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "prices = data['Close'].values\n",
    "\n",
    "# Function to create sequences for training\n",
    "def create_sequences(data, window):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i:i + window])\n",
    "        y.append(data[i + window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(prices, window_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Transformer Model Definition\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=1, num_heads=1, ffn_hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.model_dim = input_dim\n",
    "        self.pos_encoder = nn.Linear(input_dim, self.model_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.model_dim, nhead=num_heads, dim_feedforward=ffn_hid_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(self.model_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x * np.sqrt(self.model_dim)  # Scale embedding\n",
    "        x = x.permute(1, 0, 2)  # Shape to [seq_len, batch_size, features]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Shape back to [batch_size, seq_len, features]\n",
    "        return self.fc_out(x[:, -1, :]).squeeze(-1)\n",
    "\n",
    "# Initialize and prepare models\n",
    "transformer_model = TransformerModel(input_dim=1)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(-1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader setup\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training function for the Transformer model\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=100):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        training_losses.append(total_loss / len(train_loader))\n",
    "    return training_losses\n",
    "\n",
    "# Train the Transformer model\n",
    "transformer_losses = train_model(transformer_model, train_loader, optimizer, criterion)\n",
    "\n",
    "# Train Linear Regression model\n",
    "linear_model = LinearRegression().fit(X_train_scaled.reshape(-1, window_size), y_train)\n",
    "\n",
    "# Ensemble prediction function\n",
    "def weighted_ensemble(transformer_model, linear_model, X_test_tensor, X_test_scaled, transformer_weight=0.70):\n",
    "    transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        transformer_preds = transformer_model(X_test_tensor).numpy()\n",
    "    linear_preds = linear_model.predict(X_test_scaled)\n",
    "    ensemble_preds = (transformer_preds * transformer_weight) + (linear_preds * (1 - transformer_weight))\n",
    "    return ensemble_preds\n",
    "\n",
    "# Generate ensemble predictions\n",
    "ensemble_preds = weighted_ensemble(transformer_model, linear_model, X_test_tensor, X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_preds)\n",
    "print(f'Ensemble MSE: {ensemble_mse}')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, ensemble_preds)\n",
    "mae = mean_absolute_error(y_test, ensemble_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - ensemble_preds) / y_test)) * 100\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MAPE: {mape}\")\n",
    "\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Actual Values', alpha=0.7)\n",
    "plt.plot(ensemble_preds, label='Ensemble Predictions', alpha=0.7)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual Prices vs. Ensemble Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9334f",
   "metadata": {},
   "source": [
    "### Stacked Transformer and Linear Regression - Stacked Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa1ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fetch data\n",
    "data = yf.download('AAME', start='2013-01-01', end='2023-01-01')\n",
    "features = data[['Open', 'High', 'Low']]\n",
    "target = data['Close']\n",
    "\n",
    "# Prepare data\n",
    "def create_sequences(features, targets, window):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window):\n",
    "        X.append(features.iloc[i:(i + window)].values)\n",
    "        y.append(targets.iloc[i + window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(features, target, window=10)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Flatten for Linear Regression\n",
    "X_train_flat = X_train_scaled.reshape(-1, X_train.shape[1]*X_train.shape[2])\n",
    "X_val_flat = X_val_scaled.reshape(-1, X_val.shape[1]*X_val.shape[2])\n",
    "X_test_flat = X_test_scaled.reshape(-1, X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "# Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_flat, y_train)\n",
    "val_predictions_linear = linear_model.predict(X_val_flat)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_features, num_layers=1, num_heads=1, ffn_hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = nn.Linear(num_features, input_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=ffn_hid_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x *= np.sqrt(self.pos_encoder.out_features)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return self.fc_out(x[:, -1, :]).squeeze(-1)\n",
    "\n",
    "transformer_model = TransformerModel(input_dim=64, num_features=3)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "def train_transformer(model, train_loader, optimizer, criterion, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"Transformer training complete.\")\n",
    "    return model\n",
    "\n",
    "# Train Transformer\n",
    "train_transformer(transformer_model, train_loader, optimizer, criterion)\n",
    "\n",
    "# Validation predictions from Transformer\n",
    "transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_predictions_transformer = transformer_model(torch.tensor(X_val_scaled, dtype=torch.float32)).numpy()\n",
    "\n",
    "# Train meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "X_val_meta = np.column_stack((val_predictions_transformer, val_predictions_linear))\n",
    "meta_model.fit(X_val_meta, y_val)\n",
    "\n",
    "# Test predictions from Transformer and Linear Regression\n",
    "with torch.no_grad():\n",
    "    test_predictions_transformer = transformer_model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy()\n",
    "test_predictions_linear = linear_model.predict(X_test_flat)\n",
    "\n",
    "# Test meta predictions\n",
    "X_test_meta = np.column_stack((test_predictions_transformer, test_predictions_linear))\n",
    "final_predictions = meta_model.predict(X_test_meta)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, final_predictions)\n",
    "mae = mean_absolute_error(y_test, final_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - final_predictions) / y_test)) * 100\n",
    "\n",
    "print(f'Final Ensemble MSE: {mse}, MAE: {mae}, RMSE: {rmse}, MAPE: {mape}%')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='Actual Prices', alpha=0.7)\n",
    "plt.plot(final_predictions, label='Predicted Prices', alpha=0.7)\n",
    "plt.title('Test Data: Actual vs. Predicted Prices by Stacked Ensemble')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e02757",
   "metadata": {},
   "source": [
    "## Stacked Transformer and Linear Regression - Stacking Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1277a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download data\n",
    "data = yf.download('AAME', start='2013-01-01', end='2023-01-01')\n",
    "features = data[['Open', 'High', 'Low']]\n",
    "target = data['Close']\n",
    "\n",
    "# Prepare data\n",
    "def create_sequences(features, targets, window=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window):\n",
    "        X.append(features.iloc[i:(i + window)].values.flatten())\n",
    "        y.append(targets.iloc[i + window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(features, target, window=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Transformer Model wrapped for use in scikit-learn\n",
    "class SklearnTransformerWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model, epochs=100, lr=0.001):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for inputs, targets in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.view(-1), targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            predictions = self.model(X_tensor)\n",
    "            return predictions.numpy()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_features, num_layers=1, num_heads=1, ffn_hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = nn.Linear(num_features, input_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=ffn_hid_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 10, x.size(1) // 10)\n",
    "        x = self.pos_encoder(x)\n",
    "        x *= np.sqrt(self.pos_encoder.out_features)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return self.fc_out(x[:, -1, :]).squeeze(-1)\n",
    "\n",
    "transformer = TransformerModel(input_dim=64, num_features=X_train.shape[1] // 10)\n",
    "wrapped_transformer = SklearnTransformerWrapper(transformer)\n",
    "\n",
    "# Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Stacking Regressor\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=[('lr', linear_model), ('transformer', wrapped_transformer)],\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the Stacking Regressor\n",
    "stacking_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = stacking_regressor.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(f'Final Ensemble MSE: {mse}, MAE: {mae}, RMSE: {rmse}, MAPE: {mape}%')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='Actual Prices', alpha=0.7)\n",
    "plt.plot(y_pred, label='Predicted Prices', alpha=0.7)\n",
    "plt.title('Test Data: Actual vs. Predicted Prices by Stacked Ensemble')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0faae",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning -  Bayesian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8582d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the parameter search space\n",
    "param_search_space = {\n",
    "    'final_estimator__fit_intercept': [True, False],\n",
    "    'transformer__epochs': (5, 20),  # search space for number of epochs\n",
    "    'transformer__lr': (1e-5, 1e-1, 'log-uniform'),  # search space for learning rate\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization hyperparameter search\n",
    "opt = BayesSearchCV(stacking_regressor, param_search_space, cv=5, n_iter=20, scoring='neg_mean_absolute_percentage_error', n_jobs=-1)\n",
    "opt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = opt.best_params_\n",
    "best_estimator = opt.best_estimator_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_estimator.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_tuned = best_estimator.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "mape_tuned = np.mean(np.abs((y_test - y_pred_tuned) / y_test)) * 100\n",
    "print(f'Tuned Stacked Ensemble MAPE: {mape_tuned}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377c06f",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cdf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "param_search_space = {\n",
    "    'final_estimator__fit_intercept': [True, False],\n",
    "    'transformer__epochs': (5, 20),  # search space for number of epochs\n",
    "    'transformer__lr': [0.001, 0.01, 0.1]  # search space for learning rate\n",
    "}\n",
    "\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(stacking_regressor, param_search_space, cv=5, scoring='neg_mean_absolute_percentage_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_estimator.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_tuned = best_estimator.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "mape_tuned = np.mean(np.abs((y_test - y_pred_tuned) / y_test)) * 100\n",
    "print(f'Tuned Stacked Ensemble MAPE: {mape_tuned}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746c8a0",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning - RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d31e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter search space\n",
    "param_distributions = {\n",
    "    'final_estimator__fit_intercept': [True, False],\n",
    "    'transformer__epochs': randint(5, 20),  # Search space for number of epochs\n",
    "    'transformer__lr': uniform(1e-5, 1e-1)  # Search space for learning rate\n",
    "}\n",
    "\n",
    "# Perform Randomized Search hyperparameter search\n",
    "random_search = RandomizedSearchCV(\n",
    "    stacking_regressor, \n",
    "    param_distributions, \n",
    "    n_iter=20, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_absolute_percentage_error', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_estimator.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_tuned = best_estimator.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "mape_tuned = np.mean(np.abs((y_test - y_pred_tuned) / y_test)) * 100\n",
    "print(f'Tuned Stacked Ensemble MAPE: {mape_tuned}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d832409",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization - optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6b27cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    final_estimator_fit_intercept = trial.suggest_categorical('final_estimator__fit_intercept', [True, False])\n",
    "    transformer_epochs = trial.suggest_int('transformer__epochs', 5, 20)\n",
    "    transformer_lr = trial.suggest_loguniform('transformer__lr', 1e-5, 1e-1)\n",
    "\n",
    "    # Update the stacking regressor with the suggested hyperparameters\n",
    "    stacking_regressor.set_params(\n",
    "        final_estimator__fit_intercept=final_estimator_fit_intercept,\n",
    "        transformer__epochs=transformer_epochs,\n",
    "        transformer__lr=transformer_lr\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = -cross_val_score(stacking_regressor, X_train_scaled, y_train, cv=5, scoring='neg_mean_absolute_percentage_error')\n",
    "    \n",
    "    # Return the mean of the scores\n",
    "    return scores.mean()\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "best_estimator = stacking_regressor.set_params(**best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_estimator.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_tuned = best_estimator.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "mape_tuned = np.mean(np.abs((y_test - y_pred_tuned) / y_test)) * 100\n",
    "print(f'Tuned Stacked Ensemble MAPE: {mape_tuned}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
